# ============================================================================
# LITERATURE REVIEW RAG CONFIGURATION
# ============================================================================
# Starter configuration file for academic literature review RAG system
# Adapted from the personality RAG system (100% MBTI accuracy, 15ms queries)
#
# This file can be imported and customized without changing code.
# All settings are overridable via environment variables.

# ----------------------------------------------------------------------------
# DATA SOURCES
# ----------------------------------------------------------------------------
data:
  # Path to your academic PDFs (organized by phase/topic)
  pdf_path: "/Users/fadzie/Desktop/lit_rag"

  # Auto-detect phase structure from folders
  auto_detect_structure: true

  # Expected phases (adjust if your structure differs)
  phases:
    - name: "Phase 1"
      full_name: "Theoretical Foundation"
      description: "Core theoretical frameworks and conceptual foundations"

    - name: "Phase 2"
      full_name: "Sectoral & Business Transitions"
      description: "Business formation, deindustrialization, COVID impact"

    - name: "Phase 3"
      full_name: "Context & Case Studies"
      description: "Regional case studies, policy analysis, specific contexts"

    - name: "Phase 4"
      full_name: "Methodology"
      description: "Research methods, data approaches, software tools"

# ----------------------------------------------------------------------------
# PDF EXTRACTION
# ----------------------------------------------------------------------------
extraction:
  # ========================================================================
  # EXTRACTOR TYPE SELECTION
  # ========================================================================
  # Options:
  # - "academic": Section-aware extraction for academic papers (default)
  #   Detects: abstract, introduction, methods, results, discussion, conclusion
  # - "business": Business document extraction
  #   Detects: executive summary, introduction, analysis, recommendations, conclusion
  # - "generic": Simple full-text extraction without section detection
  # - "auto": Auto-detect based on document content
  #
  # For user knowledge bases (jobs), this can be overridden per-job
  # ========================================================================
  extractor_type: "academic"  # Default for the main literature collection

  # Section detection strategy (for academic/business extractors)
  use_section_detection: true
  section_confidence_threshold: 0.7  # 70% sections must be detected to use section-aware chunking

  # Sections to extract (for section-aware chunking)
  extract_sections:
    - abstract
    - introduction
    - methods
    - results
    - discussion
    - conclusion

  # Metadata to extract from each PDF
  extract_metadata:
    - title         # From PDF metadata or first page
    - authors       # From PDF metadata or parsing
    - year          # From metadata or filename
    - keywords      # From abstract/keywords section
    - doi           # If available
    - abstract      # Full abstract text

  # Fallback to simple extraction if section detection fails
  fallback_to_full_text: true

  # PDF processing settings
  max_pages_per_pdf: null  # null = no limit, or set number to limit
  skip_references: true    # Skip reference/bibliography sections
  extract_first_n_pages_for_metadata: 3  # Extract metadata from first 3 pages

  # OCR settings (for scanned documents)
  # DISABLED BY DEFAULT for performance - scanned docs add 30+ sec per page
  use_ocr_fallback: false  # Set to true if you have many scanned PDFs
  ocr_dpi: 200             # Lower DPI = faster but less accurate
  ocr_language: "eng"      # Language for OCR (eng, deu, fra, etc.)

# ----------------------------------------------------------------------------
# CHUNKING STRATEGY
# ----------------------------------------------------------------------------
chunking:
  # Primary strategy: "section_aware", "fixed_size", or "hybrid"
  strategy: "section_aware"

  # Section-specific chunk sizes (used when section detection succeeds)
  section_sizes:
    abstract: 1500      # Keep abstracts relatively intact
    introduction: 2000
    methods: 2000
    results: 2000
    discussion: 2000
    conclusion: 1500

  # Overlap for section chunks
  section_overlap: 300

  # Fallback fixed-size settings (proven in personality RAG)
  # Used when section detection fails or strategy is "fixed_size"
  fixed_chunk_size: 1000
  fixed_chunk_overlap: 200

  # Hybrid strategy (try section-aware first, fallback to fixed-size)
  hybrid_min_confidence: 0.7  # Min confidence to use section-aware

# ----------------------------------------------------------------------------
# EMBEDDING & INDEXING
# ----------------------------------------------------------------------------
embedding:
  # ========================================================================
  # EMBEDDING PROVIDER SELECTION
  # ========================================================================
  # OpenAI embeddings only.
  # IMPORTANT: Switching OpenAI models requires reindexing due to dimension differences:
  # - text-embedding-3-small: 1536 dimensions
  # - text-embedding-3-large: 3072 dimensions
  # ========================================================================
  provider: "openai"

  # ========================================================================
  # STRICT PROVIDER MODE (PRODUCTION SAFETY)
  # ========================================================================
  # If true, the system will FAIL FAST if the specified provider is unavailable
  # (e.g., provider: "openai" but OPENAI_API_KEY is missing).
  #
  # If false, it will allow non-OpenAI providers (not supported).
  #
  # RECOMMENDED: Keep true in production.
  # ========================================================================
  strict_provider: true

  # OpenAI embedding settings (used when provider: "openai")
  # Models: text-embedding-3-small (1536 dims, cheap), text-embedding-3-large (3072 dims, better)
  openai_model: "text-embedding-3-small"

  # Batch size for indexing (adjust based on available memory)
  batch_size: 32

# ----------------------------------------------------------------------------
# SEARCH & RETRIEVAL
# ----------------------------------------------------------------------------
retrieval:
  # Default number of results to return
  default_n_results: 5

  # Hybrid search (BM25 + dense embeddings)
  use_hybrid: true        # Enable BM25 + dense hybrid search
  hybrid_method: "rrf"    # "rrf" (Reciprocal Rank Fusion) or "weighted"
  hybrid_weight: 0.7      # Dense weight (only used if hybrid_method=weighted)
  bm25_candidates: 50     # Number of BM25 candidates to retrieve
  bm25_use_stemming: true # Use Porter stemming for BM25 tokenization
  bm25_min_token_length: 2  # Minimum token length for BM25

  # Cross-encoder reranking (requires sentence-transformers + torch)
  use_reranking: true
  reranker_model: "BAAI/bge-reranker-base"
  rerank_top_k: 20         # Retrieve top K candidates before reranking

  # Query expansion (academic term normalization)
  expand_queries: true
  max_expansions: 2       # Maximum number of term expansions per query

  # Distance metric for similarity search
  distance_metric: "cosine"  # Options: "cosine", "euclidean", "dot"

  # Language-aware retrieval (EN/DE)
  language_filter_enabled: true
  language_filter_fallback: true  # Retry without language filter if no results

# ----------------------------------------------------------------------------
# QUERY NORMALIZATION (KEY PATTERN FROM PERSONALITY RAG)
# ----------------------------------------------------------------------------
# This is the "secret sauce" - explicit term normalization like MBTI codes
normalization:
  # Enable academic term normalization (set true per-tenant if desired)
  enable: false

  # Custom term mappings (add your domain-specific terms here)
  term_maps:
    # Regional terms (German focus)
    regional:
      - ["ruhrgebiet", "ruhr valley", "ruhr region", "ruhr area", "ruhr metropolis"]
      - ["nrw", "north rhine-westphalia", "nordrhein-westfalen"]
      - ["rhine-ruhr", "rhine ruhr", "rhine-ruhr metropolitan"]

    # Economic concepts
    economic:
      - ["deindustrialization", "structural change", "industrial decline", "post-industrial transition"]
      - ["tertiarization", "service sector growth", "shift to services", "service economy"]
      - ["business formation", "entrepreneurship", "startup creation", "new ventures", "firm creation", "business creation", "firm births", "new firm entry"]
      - ["economic resilience", "regional resilience", "adaptive capacity"]
      - ["post-industrial transition", "industrial restructuring", "economic restructuring", "regional transition"]

    # Institutional concepts
    institutional:
      - ["voc", "varieties of capitalism", "institutional framework"]
      - ["lme", "liberal market economy", "liberal market economies"]
      - ["cme", "coordinated market economy", "coordinated market economies"]
      - ["institutional economics", "institutional theory", "new institutional economics"]

    # Methodology terms
    methodology:
      - ["spatial panel", "spatial econometrics", "spatial panel data", "spatial regression"]
      - ["mixed methods", "qualitative and quantitative", "multi-method", "triangulation"]
      - ["inkar data", "inkar indicators", "regional indicators", "bbsr data"]
      - ["case study", "case studies", "comparative case"]

    # Regional policy terms
    policy:
      - ["just transition", "fair transition", "equitable transition", "social transition"]
      - ["regional policy", "regional development policy", "structural policy"]
      - ["eu funds", "european funds", "structural funds", "cohesion policy"]

    governance:
      - ["municipal strategy", "city strategy", "local development plan", "master plan", "urban strategy", "strategic plan", "municipal strategy documents"]

# ----------------------------------------------------------------------------
# FILTERS
# ----------------------------------------------------------------------------
filters:
  # Enable various filter dimensions
  enable_phase_filter: true
  enable_topic_filter: true
  enable_year_filter: true
  enable_methodology_filter: true
  enable_geographic_filter: true
  enable_research_type_filter: true

  # Valid values for filters (auto-detected from folder structure)
  # You can customize these if your structure differs
  valid_research_types:
    - empirical
    - theoretical
    - case_study
    - mixed_methods
    - literature_review
    - methodology

  valid_geographic_focus:
    - Germany
    - Ruhr Valley
    - North Rhine-Westphalia
    - Europe
    - Global
    - Comparative

# ----------------------------------------------------------------------------
# API CONFIGURATION
# ----------------------------------------------------------------------------
api:
  # Server settings
  host: "0.0.0.0"
  port: 8001  # Different from personality RAG (8000)

  # CORS settings (for web access)
  cors_origins:
    - "http://localhost:3000"
    - "http://127.0.0.1:3000"
    - "http://localhost:5173"
    - "http://127.0.0.1:5173"
    - "http://13.49.191.201"
    - "https://13.49.191.201"
  cors_credentials: true
  cors_methods: ["GET", "POST", "PUT", "PATCH", "DELETE", "OPTIONS"]
  cors_headers: ["Authorization", "Content-Type", "X-API-Key", "X-CSRF-Token"]

  # Optional authentication
  require_api_key: false  # Disabled for local development
  api_key: null  # Set via environment variable: API_KEY=your-key

  # Rate limiting (per-tenant sliding window)
  rate_limit:
    enabled: false  # Set to true to enable rate limiting
    requests: 100   # Max requests per window (anonymous users)
    window_seconds: 60  # Time window in seconds
    authenticated_multiplier: 2.0  # Authenticated users get 2x the limit
    burst_multiplier: 1.5  # Allow short bursts up to requests * multiplier

  # API documentation
  title: "Literature Review RAG API"
  description: "Academic literature search system for German regional economic transitions"
  version: "1.0.0"

# ----------------------------------------------------------------------------
# STORAGE
# ----------------------------------------------------------------------------
storage:
  # Indices storage path (absolute path for MCP server compatibility)
  indices_path: "/Users/fadzie/Desktop/lit_rag/literature_review_rag_api/indices"

  # ChromaDB collection name
  collection_name: "literature_review_chunks"

  # Metadata caching for faster startup
  cache_metadata: true
  metadata_cache_path: "./indices/metadata_cache.pkl"

  # ChromaDB settings
  chroma_settings:
    anonymized_telemetry: false
    allow_reset: true

# ----------------------------------------------------------------------------
# AUTHENTICATION SETTINGS
# ----------------------------------------------------------------------------
# IMPORTANT: Auth is REQUIRED by default for security. The setting below
# explicitly disables it for local development. For production, remove this
# line or set to true. Can also override via AUTH_REQUIRE_AUTH=true env var.
auth:
  # JWT settings (override via environment variables for production)
  # JWT_SECRET_KEY - Set this in .env for production!
  access_token_expire_minutes: 30
  refresh_token_expire_days: 7

  # OAuth providers (set client IDs/secrets via environment variables)
  # GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET
  # GITHUB_CLIENT_ID, GITHUB_CLIENT_SECRET
  oauth_redirect_url: "http://localhost:5173/auth/callback"

  # Require authentication for API access (default: true)
  # For local dev without auth, set AUTH_REQUIRE_AUTH=false in .env
  require_auth: true

# ----------------------------------------------------------------------------
# UPLOAD SETTINGS
# ----------------------------------------------------------------------------
upload:
  # Enable PDF upload functionality
  enabled: true
  # Require S3 storage for all uploads
  s3_only: true

  # Maximum file size in bytes (50 MB default)
  max_file_size: 52428800

  # Temporary upload directory (for processing)
  temp_path: "./uploads/temp"

  # Permanent storage directory (for uploaded PDFs)
  storage_path: "./uploads/pdfs"

  # Allowed file extensions
  allowed_extensions:
    - ".pdf"

  # Auto-cleanup temp files after processing
  cleanup_temp: true

  # Processing timeout in seconds
  processing_timeout: 300

# ----------------------------------------------------------------------------
# ADVANCED FEATURES
# ----------------------------------------------------------------------------
advanced:
  # Citation network analysis (future feature)
  build_citation_network: false

  # Research gap detection
  enable_gap_analysis: true
  gap_analysis:
    min_topic_coverage: 3  # Min papers per topic to consider "covered"
    methodology_diversity_threshold: 2  # Min methodologies per phase

  # External enrichment (Semantic Scholar API, etc.)
  use_external_enrichment: false
  semantic_scholar_api_key: null

  # Logging
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  log_file: "/Users/fadzie/Desktop/lit_rag/literature_review_rag_api/logs/literature_rag.log"

  # Performance
  parallel_processing: true
  max_workers: 4  # Number of parallel PDF extraction workers

# ----------------------------------------------------------------------------
# AGENTIC RAG SETTINGS
# ----------------------------------------------------------------------------
# Adaptive query routing with multi-agent reasoning for complex queries
agentic:
  # Enable agentic pipeline (replaces basic /api/chat)
  enabled: true

  # Query classification thresholds (no LLM needed - fast regex/heuristics)
  classification:
    simple_max_words: 15       # Queries with <=15 words may be simple
    complex_min_topics: 3      # Queries mentioning 3+ topics are complex
    complex_min_words: 40      # Very long queries are likely complex

  # Quality thresholds for self-correction
  thresholds:
    evaluation_sufficient: 0.7   # Min score to proceed without retrieval retry
    citation_accuracy_min: 0.8   # Min citation accuracy to pass validation
    max_retrieval_retries: 2     # Max retrieval retry attempts
    max_regeneration_retries: 1  # Max regeneration attempts after validation failure

  # Agent-specific configurations
  agents:
    planning:
      temperature: 0.3
      max_tokens: 500
    evaluation:
      temperature: 0.1
      max_tokens: 300
    validation:
      temperature: 0.1
      max_tokens: 500
    generation:
      temperature: 0.2
      max_tokens: 2048

# ----------------------------------------------------------------------------
# CUSTOM SETTINGS
# ----------------------------------------------------------------------------
# Add your custom settings below
custom:
  # Example: thesis-specific metadata
  thesis_title: "Regional Economic Transitions in Post-Industrial Germany"
  research_focus: "Institutional Economics and Regional Development"

  # Example: favorite queries for quick testing
  test_queries:
    - "business formation in Ruhr Valley"
    - "spatial panel data methods"
    - "varieties of capitalism Germany"
    - "COVID-19 impact on manufacturing"
